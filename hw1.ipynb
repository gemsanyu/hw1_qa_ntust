{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351fe850",
   "metadata": {},
   "source": [
    "#Start\n",
    "\n",
    "simply start by reading the original data,, encode the Industry as label,, only use basic columns, remove TVC and ROE because they are highly correlated to TRC and ROA, respectively. drop rows with null columns, lazy to (properly) impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae469f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from custom_aggregator import GroupStatsAggregator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"homework_1.csv\", encoding=\"latin-1\")\n",
    "encoder = LabelEncoder()\n",
    "data[\"Industry\"]=encoder.fit_transform(data[\"Industry\"])\n",
    "data = data.drop(columns=[\"TVC\",\"ROE\",\"Yt.2M\",\"Yt.3M\",\"Code\"])\n",
    "data[\"EV\"] = data['EV'].str.replace(\",\",\"\").astype(float)\n",
    "data[\"PSR\"] = data['PSR'].str.replace(\"#DIV/0!\",\"Nan\").astype(float)\n",
    "data = data[[\"Yt.1M\",\"Industry\",\"MR\",\"TRC\",\"BAB\",\"EV\",\"P/B\",\"PSR\",\"ROA\",\"C/A\",\"D/A\",\"PG\",\"AG\"]]\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2480be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_aggregator import DataFrameWrapper\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "def prepare_model_pipeline(iterations=100,\n",
    "                           depth=3,\n",
    "                           lr=0.1,\n",
    "                           n_clusters=8,\n",
    "                           n_neighbors=3,\n",
    "                           n_features_to_select=12):\n",
    "    numeric_columns = ['MR', 'TRC', 'BAB', 'EV', 'P/B', 'PSR', 'ROA', 'C/A', 'D/A', 'PG', 'AG', 'Industry-cluster-MR-mean', 'Industry-cluster-TRC-mean', 'Industry-cluster-BAB-mean', 'Industry-cluster-EV-mean', 'Industry-cluster-P/B-mean', 'Industry-cluster-PSR-mean', 'Industry-cluster-ROA-mean', 'Industry-cluster-C/A-mean', 'Industry-cluster-D/A-mean', 'Industry-cluster-PG-mean', 'Industry-cluster-AG-mean', 'Industry-MR-mean', 'Industry-TRC-mean', 'Industry-BAB-mean', 'Industry-EV-mean', 'Industry-P/B-mean', 'Industry-PSR-mean', 'Industry-ROA-mean', 'Industry-C/A-mean', 'Industry-D/A-mean', 'Industry-PG-mean', 'Industry-AG-mean', 'cluster-MR-mean', 'cluster-TRC-mean', 'cluster-BAB-mean', 'cluster-EV-mean', 'cluster-P/B-mean', 'cluster-PSR-mean', 'cluster-ROA-mean', 'cluster-C/A-mean', 'cluster-D/A-mean', 'cluster-PG-mean', 'cluster-AG-mean']\n",
    "    transforms = [\n",
    "    ('mms', DataFrameWrapper(MinMaxScaler(), columns=numeric_columns)),\n",
    "    ('ss', DataFrameWrapper(StandardScaler(), columns=numeric_columns)),\n",
    "    ('rs', DataFrameWrapper(RobustScaler(), columns=numeric_columns)),\n",
    "    ('qt', DataFrameWrapper(QuantileTransformer(n_quantiles=100, output_distribution='normal'), columns=numeric_columns)),\n",
    "    ('kbd', DataFrameWrapper(KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform'), columns=numeric_columns)),\n",
    "    ('svd', DataFrameWrapper(TruncatedSVD(n_components=7), columns=numeric_columns)),\n",
    "    ]\n",
    "    fu = FeatureUnion(transforms).set_output(transform=\"pandas\")\n",
    "    fu = DataFrameWrapper(fu)\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', fu, numeric_columns)\n",
    "    ], remainder=\"passthrough\")\n",
    "    preprocessor.set_output(transform=\"pandas\")\n",
    "    wrapped_preprocessor = DataFrameWrapper(preprocessor)\n",
    "    rfe_estimator = CatBoostRegressor(iterations=iterations, depth=depth, learning_rate=lr, verbose=0)\n",
    "    rfe = RFE(estimator=rfe_estimator, n_features_to_select=n_features_to_select)\n",
    "\n",
    "    steps = []\n",
    "    steps.append((\"gsa\",GroupStatsAggregator(n_clusters=n_clusters, n_neighbors=n_neighbors)))\n",
    "    steps.append((\"preprocess\",wrapped_preprocessor))\n",
    "    steps.append((\"rfe\",rfe))\n",
    "    steps.append((\"regressor\",CatBoostRegressor(iterations=iterations,depth=depth,learning_rate=lr,verbose=0)))\n",
    "    model = Pipeline(steps)\n",
    "    return model\n",
    "\n",
    "model = prepare_model_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56142f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[\"Yt.1M\"])\n",
    "y = data[\"Yt.1M\"]\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718aed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part can be used to see the selected featuers\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "# kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "# # Loop over each fold for cross-validation\n",
    "# fold_metrics = []\n",
    "# fold_counter = 1\n",
    "# x = data.drop(columns=[\"Yt.1M\"])\n",
    "# y = data[\"Yt.1M\"]\n",
    "# params = {\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'learning_rate': 0.05,\n",
    "#     'num_leaves': 31,\n",
    "#     'verbose': -1\n",
    "# }\n",
    "# for train_index, test_index in kf.split(x):\n",
    "#     X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#     model = Pipeline(steps)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     fold_metrics.append({\n",
    "#     'Fold': fold_counter,\n",
    "#     'MSE': mse,\n",
    "#     'RMSE': rmse,\n",
    "#     'MAE': mae\n",
    "#     })\n",
    "#     print(f\"Fold {fold_counter} -- MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "# selector = model.named_steps['rfe']\n",
    "# fu = model.named_steps[\"preprocess\"]\n",
    "# columns = np.asanyarray(fu.final_columns.tolist())\n",
    "# selection_mask = selector.support_\n",
    "# print(columns[selection_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476566d",
   "metadata": {},
   "source": [
    "#Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6633de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    X = data.drop(columns=[\"Yt.1M\"])\n",
    "    y = data[\"Yt.1M\"]\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 8),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 50, 150),\n",
    "        \"n_clusters\": trial.suggest_int(\"n_clusters\", 4, 8),\n",
    "        \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 5),\n",
    "        \"n_features_to_select\": trial.suggest_int(\"n_features_to_select\", 5, 15),\n",
    "    }\n",
    "    model = prepare_model_pipeline(iterations=param[\"iterations\"],\n",
    "                                depth=param[\"depth\"],\n",
    "                                lr=param[\"learning_rate\"],\n",
    "                                n_clusters=param[\"n_clusters\"],\n",
    "                                n_neighbors=param[\"n_neighbors\"],\n",
    "                                n_features_to_select=param[\"n_features_to_select\"])\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    return -np.asanyarray(cv_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1cfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 00:03:28,324] A new study created in memory with name: no-name-7aecfb8b-b402-46ac-bc4a-c13eb10959b9\n",
      "[I 2025-04-15 00:06:25,357] Trial 0 finished with value: 0.12036309620828649 and parameters: {'learning_rate': 0.07592800733509816, 'depth': 6, 'iterations': 51, 'n_clusters': 6, 'n_neighbors': 4, 'n_features_to_select': 9}. Best is trial 0 with value: 0.12036309620828649.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=500)  # you can increase n_trials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rust-pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
